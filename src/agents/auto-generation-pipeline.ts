import { runQuery, runTransaction } from '../db/neo4j.js'
import { createSupabaseClient } from '../db/supabase'
import { z } from 'zod'
import puppeteer from 'puppeteer'
import sharp from 'sharp'
import { createHash } from 'crypto'
import { getOpenRouterClient, type OpenRouterClient } from '../config/openrouter.js'

// Content source types
const ContentSourceSchema = z.object({
  id: z.string(),
  source: z.enum(['github', 'twitter', 'youtube', 'blog', 'reddit', 'hackernews']),
  sourceId: z.string(),
  title: z.string(),
  content: z.string(),
  summary: z.string().optional(),
  url: z.string(),
  author: z.string(),
  publishedAt: z.date(),
  tags: z.array(z.string()),
  relevanceScore: z.number(),
  metadata: z.record(z.any()),
  processed: z.boolean(),
  mediaUrls: z.array(z.string()).optional()
})

type ContentSource = z.infer<typeof ContentSourceSchema>

// Blog post generation schema
const BlogPostSchema = z.object({
  title: z.string(),
  slug: z.string(),
  content: z.string(),
  excerpt: z.string(),
  category: z.string(),
  tags: z.array(z.string()),
  featuredImage: z.string().optional(),
  media: z.array(z.object({
    url: z.string(),
    type: z.enum(['image', 'video', 'screenshot']),
    caption: z.string().optional(),
    alt: z.string()
  })),
  metadata: z.object({
    sourceContentIds: z.array(z.string()),
    autoGenerated: z.literal(true),
    generationTimestamp: z.string(),
    qualityScore: z.number(),
    groupingMethod: z.string()
  }),
  seoMetadata: z.object({
    metaDescription: z.string(),
    keywords: z.array(z.string()),
    ogImage: z.string().optional()
  })
})

type BlogPost = z.infer<typeof BlogPostSchema>

// Content grouping strategies
interface GroupingStrategy {
  name: string
  group(contents: ContentSource[]): ContentSource[][]
}

// Pipeline configuration
interface PipelineConfig {
  supabaseUrl: string
  supabaseServiceKey: string
  openRouterApiKey: string
  screenshotStorage: string
  minQualityScore: number
  autoPublishThreshold: number
  maxContentAge: number // days
}

export class AutoGenerationPipeline {
  private supabase: ReturnType<typeof createSupabaseClient>
  private openRouter: OpenRouterClient
  private browser: puppeteer.Browser | null = null
  private config: PipelineConfig

  constructor(config: PipelineConfig) {
    this.config = config
    this.supabase = createSupabaseClient({
      SUPABASE_URL: config.supabaseUrl,
      SUPABASE_SERVICE_KEY: config.supabaseServiceKey
    })
    this.openRouter = getOpenRouterClient(config.openRouterApiKey)
  }

  /**
   * Main pipeline execution
   */
  async execute(): Promise<BlogPost[]> {
    console.log('Starting auto-generation pipeline...')
    
    try {
      // 1. Fetch queued content from all monitoring agents
      const queuedContent = await this.fetchQueuedContent()
      console.log(`Found ${queuedContent.length} queued content items`)

      if (queuedContent.length === 0) {
        return []
      }

      // 2. Group related content
      const contentGroups = await this.groupRelatedContent(queuedContent)
      console.log(`Created ${contentGroups.length} content groups`)

      // 3. Generate blog posts for each group
      const blogPosts: BlogPost[] = []
      
      for (const group of contentGroups) {
        try {
          const blogPost = await this.generateBlogPost(group)
          
          // 4. Quality check
          if (blogPost.metadata.qualityScore >= this.config.minQualityScore) {
            // 5. Capture screenshots and media
            await this.captureMediaForPost(blogPost, group)
            
            // 6. Optimize media
            await this.optimizeMedia(blogPost)
            
            // 7. Save and potentially auto-publish
            await this.saveBlogPost(blogPost)
            blogPosts.push(blogPost)
            
            // 8. Mark source content as processed
            await this.markContentAsProcessed(group)
          } else {
            console.log(`Skipping post "${blogPost.title}" due to low quality score: ${blogPost.metadata.qualityScore}`)
          }
        } catch (error) {
          console.error('Error generating blog post for group:', error)
        }
      }

      console.log(`Generated ${blogPosts.length} blog posts`)
      return blogPosts
    } finally {
      // Clean up browser if initialized
      if (this.browser) {
        await this.browser.close()
        this.browser = null
      }
    }
  }

  /**
   * Fetch queued content from all monitoring sources
   */
  private async fetchQueuedContent(): Promise<ContentSource[]> {
    // Fetch from Neo4j (innovation trackers)
    const neo4jContent = await this.fetchNeo4jContent()
    
    // Fetch from Supabase (Anthropic monitor, etc.)
    const supabaseContent = await this.fetchSupabaseContent()
    
    // Combine and deduplicate
    const allContent = [...neo4jContent, ...supabaseContent]
    return this.deduplicateContent(allContent)
  }

  /**
   * Fetch content from Neo4j database
   */
  private async fetchNeo4jContent(): Promise<ContentSource[]> {
    const query = `
      MATCH (p:Project)
      WHERE p.innovationScore > 50 
        AND NOT EXISTS(p.blogProcessed)
        AND datetime(p.lastUpdated) > datetime() - duration('P${this.config.maxContentAge}D')
      RETURN p
      ORDER BY p.innovationScore DESC
      LIMIT 50
    `

    try {
      const result = await runQuery(query)
      return result.records.map(record => {
        const project = record.get('p').properties
        return {
          id: `neo4j-${project.githubId}`,
          source: 'github' as const,
          sourceId: project.githubId.toString(),
          title: project.name,
          content: project.description || '',
          summary: project.description,
          url: project.url,
          author: project.owner,
          publishedAt: new Date(project.lastUpdated),
          tags: project.topics || [],
          relevanceScore: project.innovationScore / 100,
          metadata: {
            stars: project.stars,
            language: project.language,
            metrics: JSON.parse(project.metrics || '{}')
          },
          processed: false
        }
      })
    } catch (error) {
      console.error('Error fetching Neo4j content:', error)
      return []
    }
  }

  /**
   * Fetch content from Supabase
   */
  private async fetchSupabaseContent(): Promise<ContentSource[]> {
    try {
      const { data, error } = await this.supabase
        .from('monitoring_content')
        .select('*')
        .eq('queued_for_blog', true)
        .eq('processed', false)
        .gte('published_at', new Date(Date.now() - this.config.maxContentAge * 24 * 60 * 60 * 1000).toISOString())
        .order('relevance_score', { ascending: false })
        .limit(50)

      if (error) throw error

      return (data || []).map(item => ({
        id: `supabase-${item.id}`,
        source: item.source,
        sourceId: item.source_id,
        title: item.title,
        content: item.content,
        summary: item.summary,
        url: item.url,
        author: item.author || 'Unknown',
        publishedAt: new Date(item.published_at),
        tags: item.tags || [],
        relevanceScore: item.relevance_score,
        metadata: item.metadata || {},
        processed: false,
        mediaUrls: item.media_urls
      }))
    } catch (error) {
      console.error('Error fetching Supabase content:', error)
      return []
    }
  }

  /**
   * Group related content using multiple strategies
   */
  private async groupRelatedContent(contents: ContentSource[]): Promise<ContentSource[][]> {
    const strategies: GroupingStrategy[] = [
      new TopicGroupingStrategy(),
      new TemporalGroupingStrategy(),
      new AuthorGroupingStrategy(),
      new ThemeGroupingStrategy()
    ]

    // Apply each strategy and collect groups
    const allGroups: ContentSource[][] = []
    const usedContentIds = new Set<string>()

    for (const strategy of strategies) {
      const groups = strategy.group(
        contents.filter(c => !usedContentIds.has(c.id))
      )
      
      for (const group of groups) {
        if (group.length > 0) {
          allGroups.push(group)
          group.forEach(c => usedContentIds.add(c.id))
        }
      }
    }

    // Add remaining single items as individual groups
    const remainingContent = contents.filter(c => !usedContentIds.has(c.id))
    for (const content of remainingContent) {
      if (content.relevanceScore >= 0.7) { // Only high-relevance singles
        allGroups.push([content])
      }
    }

    return allGroups
  }

  /**
   * Generate a blog post from grouped content
   */
  private async generateBlogPost(contentGroup: ContentSource[]): Promise<BlogPost> {
    const mainContent = contentGroup[0] // Highest relevance as primary
    
    // Prepare context for AI generation
    const context = this.prepareGenerationContext(contentGroup)
    
    // Generate blog content using OpenRouter
    const generatedContent = await this.generateWithOpenRouter(context, contentGroup)
    
    // Create blog post structure
    const blogPost: BlogPost = {
      title: generatedContent.title,
      slug: this.generateSlug(generatedContent.title),
      content: generatedContent.content,
      excerpt: generatedContent.excerpt,
      category: this.determineCategory(contentGroup),
      tags: this.consolidateTags(contentGroup),
      media: [],
      metadata: {
        sourceContentIds: contentGroup.map(c => c.id),
        autoGenerated: true,
        generationTimestamp: new Date().toISOString(),
        qualityScore: await this.assessQuality(generatedContent),
        groupingMethod: this.identifyGroupingMethod(contentGroup)
      },
      seoMetadata: {
        metaDescription: generatedContent.metaDescription,
        keywords: generatedContent.keywords,
        ogImage: undefined // Will be set after screenshot capture
      }
    }

    return blogPost
  }

  /**
   * Generate content using OpenRouter API
   */
  private async generateWithOpenRouter(
    context: string, 
    contentGroup: ContentSource[]
  ): Promise<{
    title: string
    content: string
    excerpt: string
    metaDescription: string
    keywords: string[]
  }> {
    const systemPrompt = `You are an expert technical writer creating blog posts about Claude/AI innovations. You excel at explaining complex technical topics in an accessible way while maintaining accuracy and depth.`

    const userPrompt = `Based on the following source materials, create an engaging blog post:

Context and sources:
${context}

Requirements:
1. Create an engaging, informative blog post that highlights the innovation
2. Maintain proper attribution to all sources
3. Focus on the technical innovation and its impact
4. Include code examples where relevant
5. Use a professional but approachable tone
6. Structure with clear sections and headings
7. Make it SEO-friendly

Please generate:
1. A compelling title (60 chars max)
2. Full blog content in Markdown (1000-2000 words)
3. An excerpt (160 chars max)
4. Meta description for SEO (160 chars max)
5. 5-10 relevant keywords

IMPORTANT: Return your response as valid JSON with these exact keys: title, content, excerpt, metaDescription, keywords (array of strings)`

    try {
      // Use high-quality model for blog generation
      const response = await this.openRouter.complete({
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature: 0.7,
        maxTokens: 4000,
        requirements: {
          minContextLength: 32000,
          requiredCapabilities: ['text', 'code'],
          preferredQuality: 'high',
          preferredSpeed: 'fast'
        }
      })

      if (!response) {
        throw new Error('No response from OpenRouter')
      }

      // Parse JSON response
      let result
      try {
        // Clean up response in case it has markdown code blocks
        const cleanedContent = response.content
          .replace(/^```json\s*/i, '')
          .replace(/^```\s*/i, '')
          .replace(/\s*```$/i, '')
          .trim()
        
        result = JSON.parse(cleanedContent)
      } catch (parseError) {
        console.error('Error parsing OpenRouter response:', parseError)
        console.log('Raw response:', response.content)
        throw parseError
      }
      
      return {
        title: result.title || 'Claude & AI Innovation',
        content: this.enhanceMarkdownContent(result.content || '', contentGroup),
        excerpt: result.excerpt || '',
        metaDescription: result.metaDescription || result.excerpt || '',
        keywords: Array.isArray(result.keywords) ? result.keywords : []
      }
    } catch (error) {
      console.error('Error generating with OpenRouter:', error)
      // Fallback to basic generation
      return this.generateFallbackContent(contentGroup)
    }
  }

  /**
   * Capture screenshots and media for the blog post
   */
  private async captureMediaForPost(blogPost: BlogPost, contentGroup: ContentSource[]): Promise<void> {
    if (!this.browser) {
      this.browser = await puppeteer.launch({
        headless: true,
        args: ['--no-sandbox', '--disable-setuid-sandbox']
      })
    }

    const page = await this.browser.newPage()
    await page.setViewport({ width: 1200, height: 800 })

    for (const content of contentGroup) {
      try {
        // Capture screenshot of the source
        if (content.url && this.shouldCaptureScreenshot(content)) {
          const screenshot = await this.captureScreenshot(page, content.url)
          if (screenshot) {
            blogPost.media.push({
              url: screenshot.url,
              type: 'screenshot',
              caption: `Screenshot of ${content.title}`,
              alt: `${content.source} content: ${content.title}`
            })
          }
        }

        // Include existing media URLs
        if (content.mediaUrls) {
          for (const mediaUrl of content.mediaUrls) {
            blogPost.media.push({
              url: mediaUrl,
              type: 'image',
              caption: `From ${content.source}`,
              alt: content.title
            })
          }
        }
      } catch (error) {
        console.error(`Error capturing media for ${content.url}:`, error)
      }
    }

    // Set featured image
    if (blogPost.media.length > 0) {
      blogPost.featuredImage = blogPost.media[0].url
      blogPost.seoMetadata.ogImage = blogPost.media[0].url
    }

    await page.close()
  }

  /**
   * Capture screenshot of a URL
   */
  private async captureScreenshot(
    page: puppeteer.Page, 
    url: string
  ): Promise<{ url: string; path: string } | null> {
    try {
      await page.goto(url, { 
        waitUntil: 'networkidle2',
        timeout: 30000 
      })

      // Wait for content to load
      await page.waitForTimeout(2000)

      // Generate unique filename
      const hash = createHash('md5').update(url).digest('hex')
      const filename = `screenshot-${hash}-${Date.now()}.png`
      const path = `${this.config.screenshotStorage}/${filename}`

      // Take screenshot
      const screenshot = await page.screenshot({
        fullPage: false,
        type: 'png'
      })

      // Upload to storage
      const { data, error } = await this.supabase.storage
        .from('blog-media')
        .upload(path, screenshot, {
          contentType: 'image/png',
          cacheControl: '3600'
        })

      if (error) throw error

      // Get public URL
      const { data: { publicUrl } } = this.supabase.storage
        .from('blog-media')
        .getPublicUrl(path)

      return { url: publicUrl, path }
    } catch (error) {
      console.error(`Error capturing screenshot of ${url}:`, error)
      return null
    }
  }

  /**
   * Optimize media files
   */
  private async optimizeMedia(blogPost: BlogPost): Promise<void> {
    for (const media of blogPost.media) {
      if (media.type === 'screenshot' || media.type === 'image') {
        try {
          // Download the image
          const response = await fetch(media.url)
          const buffer = await response.arrayBuffer()

          // Optimize with sharp
          const optimized = await sharp(Buffer.from(buffer))
            .resize(1200, null, { 
              withoutEnlargement: true,
              fit: 'inside'
            })
            .jpeg({ quality: 85, progressive: true })
            .toBuffer()

          // Generate WebP version
          const webp = await sharp(Buffer.from(buffer))
            .resize(1200, null, { 
              withoutEnlargement: true,
              fit: 'inside'
            })
            .webp({ quality: 85 })
            .toBuffer()

          // Upload optimized versions
          const basePath = media.url.split('/').pop()?.replace(/\.[^.]+$/, '') || 'image'
          
          // Upload JPEG
          await this.supabase.storage
            .from('blog-media')
            .upload(`optimized/${basePath}.jpg`, optimized, {
              contentType: 'image/jpeg',
              cacheControl: '3600',
              upsert: true
            })

          // Upload WebP
          await this.supabase.storage
            .from('blog-media')
            .upload(`optimized/${basePath}.webp`, webp, {
              contentType: 'image/webp',
              cacheControl: '3600',
              upsert: true
            })

          // Update media URL to optimized version
          const { data: { publicUrl } } = this.supabase.storage
            .from('blog-media')
            .getPublicUrl(`optimized/${basePath}.jpg`)

          media.url = publicUrl
        } catch (error) {
          console.error('Error optimizing media:', error)
        }
      }
    }
  }

  /**
   * Save blog post to database
   */
  private async saveBlogPost(blogPost: BlogPost): Promise<void> {
    const shouldAutoPublish = blogPost.metadata.qualityScore >= this.config.autoPublishThreshold

    const { error } = await this.supabase
      .from('blog_posts')
      .insert({
        title: blogPost.title,
        slug: blogPost.slug,
        content: blogPost.content,
        excerpt: blogPost.excerpt,
        category: blogPost.category,
        tags: blogPost.tags,
        featured_image: blogPost.featuredImage,
        media: blogPost.media,
        metadata: blogPost.metadata,
        seo_metadata: blogPost.seoMetadata,
        status: shouldAutoPublish ? 'published' : 'draft',
        published_at: shouldAutoPublish ? new Date().toISOString() : null,
        created_at: new Date().toISOString(),
        updated_at: new Date().toISOString()
      })

    if (error) {
      console.error('Error saving blog post:', error)
      throw error
    }

    console.log(`Blog post "${blogPost.title}" saved as ${shouldAutoPublish ? 'published' : 'draft'}`)
  }

  /**
   * Mark source content as processed
   */
  private async markContentAsProcessed(contentGroup: ContentSource[]): Promise<void> {
    // Mark Neo4j content
    const neo4jIds = contentGroup
      .filter(c => c.id.startsWith('neo4j-'))
      .map(c => c.sourceId)

    if (neo4jIds.length > 0) {
      const query = `
        MATCH (p:Project)
        WHERE p.githubId IN $ids
        SET p.blogProcessed = true,
            p.blogProcessedAt = datetime()
      `
      await runQuery(query, { ids: neo4jIds })
    }

    // Mark Supabase content
    const supabaseIds = contentGroup
      .filter(c => c.id.startsWith('supabase-'))
      .map(c => c.sourceId)

    if (supabaseIds.length > 0) {
      await this.supabase
        .from('monitoring_content')
        .update({ 
          processed: true,
          processed_at: new Date().toISOString()
        })
        .in('source_id', supabaseIds)
    }
  }

  // Helper methods

  private deduplicateContent(contents: ContentSource[]): ContentSource[] {
    const seen = new Map<string, ContentSource>()
    
    for (const content of contents) {
      const key = `${content.source}-${content.sourceId}`
      if (!seen.has(key) || content.relevanceScore > seen.get(key)!.relevanceScore) {
        seen.set(key, content)
      }
    }

    return Array.from(seen.values())
  }

  private prepareGenerationContext(contentGroup: ContentSource[]): string {
    return contentGroup.map(content => `
Source: ${content.source}
Title: ${content.title}
URL: ${content.url}
Author: ${content.author}
Published: ${content.publishedAt.toLocaleDateString()}
Summary: ${content.summary || content.content.substring(0, 500)}
Tags: ${content.tags.join(', ')}
${content.metadata.stars ? `Stars: ${content.metadata.stars}` : ''}
---
    `).join('\n')
  }

  private generateSlug(title: string): string {
    const date = new Date().toISOString().split('T')[0]
    const slug = title
      .toLowerCase()
      .replace(/[^a-z0-9]+/g, '-')
      .replace(/^-+|-+$/g, '')
      .substring(0, 60)
    
    return `${date}-${slug}`
  }

  private determineCategory(contentGroup: ContentSource[]): string {
    const allTags = contentGroup.flatMap(c => c.tags)
    const tagFrequency = new Map<string, number>()
    
    for (const tag of allTags) {
      tagFrequency.set(tag, (tagFrequency.get(tag) || 0) + 1)
    }

    // Category mapping
    const categoryMap: Record<string, string[]> = {
      'Tools & Extensions': ['extension', 'tool', 'cli', 'mcp', 'integration'],
      'Development': ['development', 'sdk', 'api', 'framework', 'library'],
      'AI Innovation': ['ai', 'llm', 'agent', 'automation', 'orchestration'],
      'Community': ['community', 'open-source', 'contribution', 'collaboration'],
      'Updates': ['update', 'release', 'announcement', 'news']
    }

    let bestCategory = 'General'
    let bestScore = 0

    for (const [category, keywords] of Object.entries(categoryMap)) {
      const score = keywords.reduce((sum, keyword) => 
        sum + (tagFrequency.get(keyword) || 0), 0
      )
      if (score > bestScore) {
        bestScore = score
        bestCategory = category
      }
    }

    return bestCategory
  }

  private consolidateTags(contentGroup: ContentSource[]): string[] {
    const tagSet = new Set<string>()
    
    for (const content of contentGroup) {
      content.tags.forEach(tag => tagSet.add(tag))
    }

    // Add auto-generation tag
    tagSet.add('auto-generated')
    tagSet.add('ai-curated')

    return Array.from(tagSet).slice(0, 15) // Limit tags
  }

  private async assessQuality(generatedContent: any): Promise<number> {
    let score = 0

    // Content length check
    const wordCount = generatedContent.content.split(/\s+/).length
    if (wordCount >= 800) score += 20
    else if (wordCount >= 500) score += 10

    // Structure check (headers, sections)
    const headerCount = (generatedContent.content.match(/^#{1,3}\s/gm) || []).length
    if (headerCount >= 3) score += 15

    // Code examples check
    const codeBlocks = (generatedContent.content.match(/```/g) || []).length / 2
    if (codeBlocks >= 1) score += 15

    // Links and references
    const links = (generatedContent.content.match(/\[.*?\]\(.*?\)/g) || []).length
    if (links >= 3) score += 10

    // SEO optimization
    if (generatedContent.metaDescription && generatedContent.metaDescription.length >= 120) score += 10
    if (generatedContent.keywords && generatedContent.keywords.length >= 5) score += 10

    // Title quality
    if (generatedContent.title.length >= 30 && generatedContent.title.length <= 60) score += 10

    // Excerpt quality
    if (generatedContent.excerpt.length >= 100 && generatedContent.excerpt.length <= 160) score += 10

    return Math.min(score, 100)
  }

  private identifyGroupingMethod(contentGroup: ContentSource[]): string {
    if (contentGroup.length === 1) return 'single-source'
    
    // Check if all from same source
    const sources = new Set(contentGroup.map(c => c.source))
    if (sources.size === 1) return `same-source-${sources.values().next().value}`

    // Check if all from same author
    const authors = new Set(contentGroup.map(c => c.author))
    if (authors.size === 1) return 'same-author'

    // Check temporal grouping
    const dates = contentGroup.map(c => c.publishedAt.toDateString())
    if (new Set(dates).size === 1) return 'same-day'

    // Check topic similarity
    const commonTags = contentGroup[0].tags.filter(tag =>
      contentGroup.every(c => c.tags.includes(tag))
    )
    if (commonTags.length >= 2) return 'topic-based'

    return 'mixed'
  }

  private shouldCaptureScreenshot(content: ContentSource): boolean {
    // Don't capture screenshots for certain sources
    if (content.source === 'twitter') return false
    if (content.source === 'blog' && content.url.includes('anthropic.com')) return false
    
    // Capture for GitHub repos and external blogs
    return true
  }

  private enhanceMarkdownContent(content: string, contentGroup: ContentSource[]): string {
    // Add source attribution section
    const attribution = `
## Sources and Attribution

This post was automatically generated from the following sources:

${contentGroup.map(c => `- [${c.title}](${c.url}) by ${c.author} (${c.publishedAt.toLocaleDateString()})`).join('\n')}
`

    // Add discovery note
    const discoveryNote = `
---

*This content was automatically discovered and curated by the Claude Extensions innovation tracking system. For more cutting-edge Claude and AI innovations, visit [claude.directory](https://claude.directory).*
`

    return content + '\n\n' + attribution + '\n' + discoveryNote
  }

  private async generateFallbackContent(contentGroup: ContentSource[]): Promise<any> {
    const mainContent = contentGroup[0]
    
    return {
      title: this.generateFallbackTitle(contentGroup),
      content: this.generateFallbackMarkdown(contentGroup),
      excerpt: mainContent.summary || mainContent.content.substring(0, 160),
      metaDescription: `Discover ${mainContent.title} and related innovations in Claude and AI development.`,
      keywords: Array.from(new Set(contentGroup.flatMap(c => c.tags))).slice(0, 10)
    }
  }

  private generateFallbackTitle(contentGroup: ContentSource[]): string {
    if (contentGroup.length === 1) {
      return contentGroup[0].title.substring(0, 60)
    }
    
    const date = new Date().toLocaleDateString('en-US', { month: 'long', year: 'numeric' })
    return `Claude & AI Innovations - ${date}`
  }

  private generateFallbackMarkdown(contentGroup: ContentSource[]): string {
    let markdown = ''

    if (contentGroup.length === 1) {
      const content = contentGroup[0]
      markdown = `# ${content.title}\n\n`
      markdown += `*Originally published on ${content.source} by ${content.author}*\n\n`
      markdown += content.content + '\n\n'
      markdown += `[View original →](${content.url})\n\n`
    } else {
      markdown = `# Latest Claude & AI Innovations\n\n`
      markdown += `We've discovered ${contentGroup.length} exciting innovations in the Claude ecosystem:\n\n`
      
      for (const content of contentGroup) {
        markdown += `## ${content.title}\n\n`
        markdown += `*via ${content.source} by ${content.author}*\n\n`
        markdown += (content.summary || content.content.substring(0, 300)) + '...\n\n'
        markdown += `[Read more →](${content.url})\n\n---\n\n`
      }
    }

    return markdown
  }
}

// Grouping Strategy Implementations

class TopicGroupingStrategy implements GroupingStrategy {
  name = 'topic-based'

  group(contents: ContentSource[]): ContentSource[][] {
    const groups: ContentSource[][] = []
    const used = new Set<string>()

    for (const content of contents) {
      if (used.has(content.id)) continue

      const group = [content]
      used.add(content.id)

      // Find related by common tags
      for (const other of contents) {
        if (used.has(other.id)) continue

        const commonTags = content.tags.filter(tag => other.tags.includes(tag))
        if (commonTags.length >= 3) {
          group.push(other)
          used.add(other.id)
        }
      }

      if (group.length >= 2) {
        groups.push(group)
      } else {
        used.delete(content.id) // Release for other strategies
      }
    }

    return groups
  }
}

class TemporalGroupingStrategy implements GroupingStrategy {
  name = 'temporal'

  group(contents: ContentSource[]): ContentSource[][] {
    const groups: ContentSource[][] = []
    const used = new Set<string>()

    // Group by same day
    const byDate = new Map<string, ContentSource[]>()
    
    for (const content of contents) {
      if (used.has(content.id)) continue
      
      const dateKey = content.publishedAt.toDateString()
      if (!byDate.has(dateKey)) {
        byDate.set(dateKey, [])
      }
      byDate.get(dateKey)!.push(content)
    }

    // Create groups from dates with multiple items
    for (const [date, items] of byDate) {
      if (items.length >= 2) {
        groups.push(items)
        items.forEach(item => used.add(item.id))
      }
    }

    return groups
  }
}

class AuthorGroupingStrategy implements GroupingStrategy {
  name = 'author-based'

  group(contents: ContentSource[]): ContentSource[][] {
    const groups: ContentSource[][] = []
    const used = new Set<string>()

    // Group by author
    const byAuthor = new Map<string, ContentSource[]>()
    
    for (const content of contents) {
      if (used.has(content.id)) continue
      
      const author = content.author.toLowerCase()
      if (!byAuthor.has(author)) {
        byAuthor.set(author, [])
      }
      byAuthor.get(author)!.push(content)
    }

    // Create groups from authors with multiple high-relevance items
    for (const [author, items] of byAuthor) {
      const highRelevance = items.filter(item => item.relevanceScore >= 0.7)
      if (highRelevance.length >= 2) {
        groups.push(highRelevance)
        highRelevance.forEach(item => used.add(item.id))
      }
    }

    return groups
  }
}

class ThemeGroupingStrategy implements GroupingStrategy {
  name = 'theme-based'

  private themes = {
    'mcp-innovation': ['mcp', 'model-context-protocol', 'context', 'protocol'],
    'claude-tools': ['tool', 'extension', 'cli', 'integration'],
    'ai-agents': ['agent', 'autonomous', 'multi-agent', 'orchestration'],
    'development': ['sdk', 'api', 'framework', 'library', 'development']
  }

  group(contents: ContentSource[]): ContentSource[][] {
    const groups: ContentSource[][] = []
    const used = new Set<string>()

    for (const [theme, keywords] of Object.entries(this.themes)) {
      const themeGroup: ContentSource[] = []

      for (const content of contents) {
        if (used.has(content.id)) continue

        const text = `${content.title} ${content.content} ${content.tags.join(' ')}`.toLowerCase()
        const matchCount = keywords.filter(keyword => text.includes(keyword)).length

        if (matchCount >= 2) {
          themeGroup.push(content)
          used.add(content.id)
        }
      }

      if (themeGroup.length >= 2) {
        groups.push(themeGroup)
      } else {
        // Release items for other strategies
        themeGroup.forEach(item => used.delete(item.id))
      }
    }

    return groups
  }
}

// Export factory function
export function createAutoGenerationPipeline(config: PipelineConfig) {
  return new AutoGenerationPipeline(config)
}

// Scheduled pipeline execution
export async function runScheduledPipeline(env: any) {
  const pipeline = createAutoGenerationPipeline({
    supabaseUrl: env.SUPABASE_URL,
    supabaseServiceKey: env.SUPABASE_SERVICE_KEY,
    openRouterApiKey: env.OPENROUTER_API_KEY,
    screenshotStorage: 'screenshots',
    minQualityScore: 60,
    autoPublishThreshold: 80,
    maxContentAge: 7
  })

  try {
    console.log('Starting scheduled blog generation pipeline...')
    const posts = await pipeline.execute()
    
    return {
      success: true,
      postsGenerated: posts.length,
      posts: posts.map(p => ({
        title: p.title,
        slug: p.slug,
        category: p.category,
        qualityScore: p.metadata.qualityScore,
        status: p.metadata.qualityScore >= 80 ? 'published' : 'draft'
      }))
    }
  } catch (error) {
    console.error('Pipeline execution failed:', error)
    return {
      success: false,
      error: error.message
    }
  }
}